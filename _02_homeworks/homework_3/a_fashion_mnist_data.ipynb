{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajhaj\\git\\link_dl\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = str(Path.cwd().resolve().parent.parent)\n",
    "print(BASE_PATH)\n",
    "\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "from _01_code._99_common_utils.utils import get_num_cpu_cores, is_linux, is_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.28604060643513995\n",
      "Std: 0.3204533660888672\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 평균(mean)과 표준편차(std)를 계산하는 함수\n",
    "def calculate_mean_std(dataset):\n",
    "    data_loader = DataLoader(dataset, batch_size=1024, shuffle=False)  # 데이터 로더 생성 (배치 크기: 1024)\n",
    "\n",
    "    mean = 0.0  # 평균 초기화\n",
    "    std = 0.0  # 표준편차 초기화\n",
    "    total_samples = 0  # 총 샘플 수 초기화\n",
    "\n",
    "    for data, _ in data_loader:  # 데이터셋에서 배치 단위로 데이터를 가져옴\n",
    "        batch_samples = data.size(0)  # 현재 배치의 샘플 수\n",
    "        data = data.view(batch_samples, -1)  # 이미지를 1D 벡터로 변환 (예: [28,28] -> [784])\n",
    "\n",
    "        mean += data.mean(1).sum().item()  # 현재 배치의 평균을 계산하고 누적\n",
    "        std += data.std(1).sum().item()  # 현재 배치의 표준편차를 계산하고 누적\n",
    "        total_samples += batch_samples  # 총 샘플 수 누적\n",
    "\n",
    "    mean /= total_samples  # 총 샘플 수로 평균 계산\n",
    "    std /= total_samples  # 총 샘플 수로 표준편차 계산\n",
    "\n",
    "    return mean, std  # 최종 계산된 평균과 표준편차 반환\n",
    "\n",
    "# FashionMNIST 데이터셋 로드\n",
    "data_path = \"./j_fashion_mnist\"  # 데이터셋 경로\n",
    "f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# 평균(mean)과 표준편차(std) 계산\n",
    "mean, std = calculate_mean_std(f_mnist_train)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # ResNet 구조에 맞게 이미지 크기 조정\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.286, std=0.320),\n",
    "    ])\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform)\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "    print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "    print(\"Sample Shape: \", f_mnist_train[0][0].shape)\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "    print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_test_data():\n",
    "    data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.286, std=0.320),\n",
    "    ])\n",
    "\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "    print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "    print(\"Sample Shape: \", f_mnist_test[0][0].shape)\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=wandb.config.batch_size)\n",
    "\n",
    "    return f_mnist_test_images, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model():\n",
    "    class Residual(nn.Module):\n",
    "        \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "        def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.LazyConv2d(out_channels=num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "            self.conv2 = nn.LazyConv2d(out_channels=num_channels, kernel_size=3, padding=1)\n",
    "            if use_1x1conv:\n",
    "                self.conv3 = nn.LazyConv2d(out_channels=num_channels, kernel_size=1, stride=strides)\n",
    "            else:\n",
    "                self.conv3 = None\n",
    "            self.bn1 = nn.LazyBatchNorm2d()\n",
    "            self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "        def forward(self, X):\n",
    "            Y = torch.relu(self.bn1(self.conv1(X)))\n",
    "            Y = self.bn2(self.conv2(Y))\n",
    "            if self.conv3:\n",
    "                X = self.conv3(X)\n",
    "            Y += X\n",
    "            return torch.relu(Y)\n",
    "\n",
    "    class ResNet(nn.Module):\n",
    "        def __init__(self, arch, n_outputs=10):\n",
    "            super(ResNet, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Sequential(\n",
    "                    nn.LazyConv2d(out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.LazyBatchNorm2d(),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for i, (num_residuals, num_channels) in enumerate(arch):\n",
    "                self.model.add_module(\n",
    "                    name=f'block_{i}', module=self.block(num_residuals, num_channels, first_block=(i == 0))\n",
    "                )\n",
    "\n",
    "            self.model.add_module(\n",
    "                name='last',\n",
    "                module=nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.LazyLinear(n_outputs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        def block(self, num_residuals, num_channels, first_block=False):\n",
    "            blk = []\n",
    "            for i in range(num_residuals):\n",
    "                if i == 0 and not first_block:\n",
    "                    blk.append(Residual(num_channels=num_channels, use_1x1conv=True, strides=2))\n",
    "                else:\n",
    "                    blk.append(Residual(num_channels=num_channels))\n",
    "            return nn.Sequential(*blk)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    my_model = ResNet(arch=((2, 64), (2, 128), (2, 256), (2, 512)), n_outputs=10)\n",
    "\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "        # Logging to wandb\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Training Loss\": train_loss / len(train_loader),\n",
    "            \"Validation Loss\": val_loss / len(val_loader),\n",
    "            \"Training Accuracy\": train_accuracy,\n",
    "            \"Validation Accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "            f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "            f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "            f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "            f\"Val Acc: {val_accuracy:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "        # Logging to wandb\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Training Loss\": train_loss / len(train_loader),\n",
    "            \"Validation Loss\": val_loss / len(val_loader),\n",
    "            \"Training Accuracy\": train_accuracy,\n",
    "            \"Validation Accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "            f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "            f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "            f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "            f\"Val Acc: {val_accuracy:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    wandb.log({\"Test Accuracy\": accuracy})\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, test_dataset, device):\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(test_dataset)), 10)\n",
    "    images = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            image, label = test_dataset[idx]\n",
    "            images.append(image.squeeze(0).cpu().numpy())\n",
    "            true_labels.append(label)\n",
    "\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            output = model(image)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted_labels.append(predicted.item())\n",
    "\n",
    "    # 이미지 출력 및 결과 비교\n",
    "    for i in range(10):\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(f\"Label: {class_labels[true_labels[i]]}\\nPredicted: {class_labels[predicted_labels[i]]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        if true_labels[i] == predicted_labels[i]:\n",
    "            print(f\"Correct Prediction for Image {i+1}\")\n",
    "        else:\n",
    "            print(f\"Incorrect Prediction for Image {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bra18zit) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁█</td></tr><tr><td>Training Accuracy</td><td>▁█</td></tr><tr><td>Training Loss</td><td>█▁</td></tr><tr><td>Validation Accuracy</td><td>▁█</td></tr><tr><td>Validation Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2</td></tr><tr><td>Training Accuracy</td><td>90.41818</td></tr><tr><td>Training Loss</td><td>0.26392</td></tr><tr><td>Validation Accuracy</td><td>91.56</td></tr><tr><td>Validation Loss</td><td>0.23454</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-11-22_20-52-38</strong> at: <a href='https://wandb.ai/SummerPillow/fashion-mnist-resnet/runs/bra18zit' target=\"_blank\">https://wandb.ai/SummerPillow/fashion-mnist-resnet/runs/bra18zit</a><br/> View project at: <a href='https://wandb.ai/SummerPillow/fashion-mnist-resnet' target=\"_blank\">https://wandb.ai/SummerPillow/fashion-mnist-resnet</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241122_205239-bra18zit\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bra18zit). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ajhaj\\git\\link_dl\\_02_homeworks\\homework_3\\wandb\\run-20241123_000517-o2hqrmbm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/SummerPillow/fashion-mnist-resnet/runs/o2hqrmbm' target=\"_blank\">2024-11-23_00-05-17</a></strong> to <a href='https://wandb.ai/SummerPillow/fashion-mnist-resnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/SummerPillow/fashion-mnist-resnet' target=\"_blank\">https://wandb.ai/SummerPillow/fashion-mnist-resnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/SummerPillow/fashion-mnist-resnet/runs/o2hqrmbm' target=\"_blank\">https://wandb.ai/SummerPillow/fashion-mnist-resnet/runs/o2hqrmbm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 64, 64])\n",
      "Number of Data Loading Workers: 16\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 64, 64])\n",
      "\n",
      "Model Summary:\n",
      "Epoch 1/10, Train Loss: 0.3945, Val Loss: 0.2871, Train Acc: 85.67%, Val Acc: 89.48%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 테스트\u001b[39;00m\n\u001b[0;32m     37\u001b[0m all_labels, all_preds \u001b[38;5;241m=\u001b[39m test_model(model, test_loader, device)\n",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m total_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     12\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 305, in check_stop_status\n",
      "    self._loop_check_status(\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 235, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 896, in deliver_stop_status\n",
      "    return self._deliver_stop_status(status)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 487, in _deliver_stop_status\n",
      "    return self._deliver_record(record)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 452, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 225, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 157, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 154, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 132, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionResetError: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다\n",
      "Exception in thread IntMsgThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 325, in check_internal_messages\n",
      "    self._loop_check_status(\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 235, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 912, in deliver_internal_messages\n",
      "    return self._deliver_internal_messages(internal_message)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 503, in _deliver_internal_messages\n",
      "    return self._deliver_record(record)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 452, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 225, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 157, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 154, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"c:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 132, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionResetError: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 현재 시간 기록\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    # 설정\n",
    "    config = {'batch_size': 64, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 1e-4}\n",
    "    wandb.init(\n",
    "        mode=\"online\",\n",
    "        project=\"fashion-mnist-resnet\",\n",
    "        notes=\"Fashion MNIST with Custom ResNet and Regularization\",\n",
    "        tags=[\"ResNet\", \"FashionMNIST\", \"Dropout\", \"Weight Decay\"],\n",
    "        name=current_time_str,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 데이터 로드\n",
    "    train_loader, val_loader = get_fashion_mnist_data()\n",
    "    test_dataset, test_loader = get_fashion_mnist_test_data()\n",
    "\n",
    "    # 모델 초기화\n",
    "    model = get_resnet_model()\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(\"\\nModel Summary:\")\n",
    "    summary(model, input_size=(wandb.config.batch_size, 1, 64, 64), device=device.type)\n",
    "\n",
    "    # 손실 함수 및 옵티마이저 정의\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    # 학습\n",
    "    train_model(model, train_loader, val_loader, config['epochs'], criterion, optimizer, device)\n",
    "\n",
    "    # 테스트\n",
    "    all_labels, all_preds = test_model(model, test_loader, device)\n",
    "\n",
    "    # 예측 결과 시각화\n",
    "    visualize_predictions(model, test_dataset, device)\n",
    "\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
