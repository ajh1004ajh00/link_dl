{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[요구사항 1]\n",
    "---\n",
    "titanic_dataset.py 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/bg6gou1o?nw=nwuserajhajh503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset): # 데이터 셋을 상속받아 데이터를 load 하고 처리하기 쉽게 해줌\n",
    "  def __init__(self, X, y):\n",
    "    self.X = torch.FloatTensor(X) # floattensor로 변환해 저장하고 feature와 target 받음.\n",
    "    self.y = torch.LongTensor(y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    target = self.y[idx]\n",
    "    return {'input': feature, 'target': target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicTestDataset(Dataset): # Test dataset에는 y 즉 target 값이 없으므로\n",
    "  def __init__(self, X): # x(입력) 값만 처리\n",
    "    self.X = torch.FloatTensor(X)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    return {'input': feature}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "      len(self.X), self.X.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclass별 Fare 평균값을 사용하여 Fare 결측치 메우기\n",
    "    #all_df[[\"Pclass\", \"Fare\"]] = 데이터셋에서 pclass, fare 열만 선택\n",
    "    #groupby(\"Pclass\") = pclass 열 기준으로 데이터를 그룹화\n",
    "    #즉, 1,2,3 등급별로 그룹이 만들어짐.\n",
    "    #mean()은 평균\n",
    "    #reset_index()은 열로 변환하여 데이터프레임 형태로 만들어줌\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    \n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]#열이름을 pclass, fare_mean으로 변경\n",
    "    #all_df 원래 데이터셋과 방금 만든 fare_mean 데이터셋을 결합\n",
    "    #on=\"Pclass\"열 기준으로 병합. left 즉 왼쪽 모든 데이터를 유지하면서 병합\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    #all_df[\"Fare\"].isnull() Fare 값이 없는 데이터를 찾아\n",
    "    #loc으로 맞는 행을 선택하고, \n",
    "    # #all_df[\"Fare\"] = all_df[\"Fare_mean\"] 없는 Fare 값을 Fare_mean 값으로 대체\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # name을 세 개의 컬럼으로 분리하여 다시 all_df에 합침\n",
    "    #학습할 때 필요없는 특징\n",
    "    #다만 나이의 평균 값을 구할 때, 존칭을 이용해 평균 나이값을 구하기 때문에\n",
    "    #honorific은 의미 있는 특징이 됨\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # honorific별 Age 평균값을 사용하여 Age 결측치 메우기\n",
    "    # 존칭(Mr,Mrs)으로 그룹을 나눠 없는 데이터 셋에 평균 나이를 추가.\n",
    "    #방식은 fare와 유사.\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "\n",
    "    # drop([\"honorific_age_mean\"], axis=1) = 해당 열 삭제.    \n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족수(family_num) 컬럼 새롭게 추가\n",
    "    #가족의 수 = 부모와 자녀 + 형제자매와 배우자 수\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "\n",
    "    # 혼자탑승(alone) 컬럼 새롭게 추가\n",
    "    #가족수가 0이면 alone 새 열을 만들어 1 할당\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    #alone 값이 0 즉 결측값이면 0으로 채워줌\n",
    "    #all_df[\"alone\"].fillna(0, inplace=True)\n",
    "    all_df[\"alone\"] = all_df[\"alone\"].fillna(0)\n",
    "\n",
    "    # 학습에 불필요한 컬럼 제거 \n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # honorific 값 개수 줄이기\n",
    "    # ~(...)은 부정 연산자로 안에 조건이 들어감.\n",
    "    # 조건을 만족하지 않는 경우 other로 대체됨\n",
    "    all_df.loc[\n",
    "    ~(\n",
    "            (all_df[\"honorific\"] == \"Mr\") |\n",
    "            (all_df[\"honorific\"] == \"Miss\") |\n",
    "            (all_df[\"honorific\"] == \"Mrs\") |\n",
    "            (all_df[\"honorific\"] == \"Master\")\n",
    "    ),\n",
    "    \"honorific\"\n",
    "    ] = \"other\"\n",
    "\n",
    "    #embarked 값이 없는경우 missing이라는 값을 할당. \n",
    "    #all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "    all_df[\"Embarked\"] = all_df[\"Embarked\"].fillna(\"missing\")\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#카테고리별 변수들을 수치화 값으로 변환하는 함수\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 범주형 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
    "    # all_df.columns 모든 열을 반환해서\n",
    "    #object 즉 문자열인 열만 모아서 category_features에 저장\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    #범주형 데이터를 수치형으로 반환해주는 라이브러리\n",
    "    #각 범주형 데이터들을 고유한 정수로 변환해주는 for문\n",
    "    for category_feature in category_features: # 범주형 데이터들을 하나씩 뽑아서\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        #범주형 데이터면\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "          #고유한 정수로 맵핑\n",
    "          le = le.fit(all_df[category_feature])\n",
    "          #범주형 데이터를 수치형으로 변환.\n",
    "          all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터의 전처리와 분할을 담당하는 파트.\n",
    "def get_preprocessed_dataset():\n",
    "    #CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "    # __file__변수는 스크립트 파일 경로이지만 쥬피터 노트북으로 변경했기 때문에\n",
    "    #자동으로 정의되지 않음. 따라서 현재 작업 디렉토리를 직접 설정해줘야함.\n",
    "    CURRENT_FILE_PATH = os.getcwd()  # 현재 디렉토리 경로 사용\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\") # 파일을 연결\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path) # 데이터 load\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    #한번에 전처리 하기 위해서 단순히 뒤에 이어 붙이는 concat 사용\n",
    "    #train에는 label인 survived가 있지만 test에는 없는걸 고려해서 처리해야함. \n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_1(all_df) #fare 결측값 처리리\n",
    "\n",
    "    all_df = get_preprocessed_dataset_2(all_df) #이름 분리\n",
    "\n",
    "    all_df = get_preprocessed_dataset_3(all_df) # age 결측값 추가가\n",
    "\n",
    "    all_df = get_preprocessed_dataset_4(all_df) # 가족, 필요없는 특징 제거\n",
    "\n",
    "    all_df = get_preprocessed_dataset_5(all_df) # embarked 결측값 추가 및 honorific 값 개수 줄이기\n",
    "\n",
    "    all_df = get_preprocessed_dataset_6(all_df) # 범주형 특징값들을 고유한 수치로 맵핑\n",
    "\n",
    "    #concat으로 붙여놨던 train 데이터와 test 데이터를 분리하는 작업\n",
    "    # train 데이터를 validation 데이터도 분리\n",
    "    #survived 값이 결측값 = test 데이터 이므로 부정 연산자를 사용해\n",
    "    # survived 값이 있는 데이터를 train x에 추가.\n",
    "    #drop을 사용해 target 인 survived를 제거\n",
    "    #reset_index(drop=True) 기존 인덱스에 새로운 열을 추가하지 않고 완전 제거 후 인덱스 재설정\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    # ~ 부정 연산자만 빼서 survived가 없는 데이터를 test x에 추가.\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    # 입력과 출력(target)으로 데이터 셋 구성\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    #print(dataset)\n",
    "    #데이터 셋을 8:2로 train 데이터와 validation 데이터를 분리\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "    #print(test_dataset)\n",
    "    # 데이터 셋을 train, validation, test로 분리후 반환\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 구조 정의하는 파트\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  #n_input = 입력 특징의 개수\n",
    "  #n_output = survived 여부 즉 0 or 1\n",
    "  def __init__(self, n_input, n_output): # 모델 구조 정의\n",
    "    super().__init__()\n",
    "\n",
    "    #순차적으로 레이어를 정의\n",
    "    #FCL이고 활성화 함수는 Relu 사용\n",
    "    # x의 특징 수 11를 입력으로 받아\n",
    "    # 30개로 선형 변환 후 히든레이어 하나 더 지나 0 or 1이 출력됨\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(n_input, 30),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(30, 30),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(30, n_output),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data_loader):\n",
    "  print(\"[TEST]\")\n",
    "  #데이터를 배치 단위로 반복\n",
    "  batch = next(iter(test_data_loader))\n",
    "  print(\"{0}\".format(batch['input'].shape))\n",
    "  #print(\"{0}\".format(batch['input']))\n",
    "  # 특징 수 11개와 0 or 1을 출력하는 모델 생성\n",
    "  my_model = MyModel(n_input=11, n_output=2)\n",
    "\n",
    "  #입력데이터를 넣어 예측을 수행.\n",
    "  output_batch = my_model(batch['input'])\n",
    "  #dim 1 즉, 0 or 1 중 더 예측값이 높은거 하나 선택\n",
    "  prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "\n",
    "  #예측한 값 출력\n",
    "  #for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "  #    print(idx, prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\ajhaj\\\\git\\\\link_dl\\\\_04_homeworks_solution\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;66;03m# 실행\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m   train_dataset, validation_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_preprocessed_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataset: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, validation_dataset.shape: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, test_dataset: \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mlen\u001b[39m(train_dataset), \u001b[38;5;28mlen\u001b[39m(validation_dataset), \u001b[38;5;28mlen\u001b[39m(test_dataset)\n\u001b[0;32m      6\u001b[0m   ))\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mget_preprocessed_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m train_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CURRENT_FILE_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 파일을 연결\u001b[39;00m\n\u001b[0;32m      8\u001b[0m test_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CURRENT_FILE_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 데이터 load\u001b[39;00m\n\u001b[0;32m     11\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_data_path)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#한번에 전처리 하기 위해서 단순히 뒤에 이어 붙이는 concat 사용\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#train에는 label인 survived가 있지만 test에는 없는걸 고려해서 처리해야함. \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ajhaj\\anaconda3\\envs\\link_dl\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\ajhaj\\\\git\\\\link_dl\\\\_04_homeworks_solution\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": # 실행\n",
    "  train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "  print(\"train_dataset: {0}, validation_dataset.shape: {1}, test_dataset: {2}\".format(\n",
    "    len(train_dataset), len(validation_dataset), len(test_dataset)\n",
    "  ))\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(train_dataset):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, sample['input'], sample['target']))\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "  # 잘 만들어진 데이터를 load\n",
    "  train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "  validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True)\n",
    "  test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "  print(\"[TRAIN]\")\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "  print(\"[VALIDATION]\")\n",
    "  for idx, batch in enumerate(validation_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  test(test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[요구사항 2]\n",
    "----\n",
    "titanic 딥러닝 모델 훈련 코드 및 activation function 변경해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #wandb.config.n_hidden_unit_list[0] 에서 설정된 첫 번째 히든레이어 노드 수\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_optimizer():\n",
    "    my_model = MyModel(n_input=11, n_output=2)\n",
    "\n",
    "    #스토캐스틱 그라디언트 디센트로 모델 학습, 러닝 레이트는 wandb.config.learning_rate\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "    return my_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs # 반복 횟수\n",
    "    loss_fn = nn.CrossEntropyLoss()  # 이진 분류 문제이므로 CrossEntropyLoss 사용\n",
    "    next_print_epoch = 100 # 100번마다 loss 출력\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        num_trains = 0\n",
    "        for train_batch in train_data_loader:\n",
    "            #target을 예측하는 학습 수행\n",
    "            input, target = train_batch['input'], train_batch['target']\n",
    "            output_train = model(input)\n",
    "            #loss 계산\n",
    "            loss = loss_fn(output_train, target)\n",
    "            loss_train += loss.item()\n",
    "            num_trains += 1\n",
    "\n",
    "            #그라디언트 초기화, 그라디언트가 초기화를 자동으로 안해줌\n",
    "            optimizer.zero_grad()\n",
    "            # 그라디언트 계산\n",
    "            loss.backward()\n",
    "            #SGD\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_validation = 0.0\n",
    "        num_validations = 0\n",
    "\n",
    "        #computation graph x\n",
    "        #validation 계산, train과 동일한 알고리즘이지만 업데이트는 하지 않음.\n",
    "        with torch.no_grad():\n",
    "            for validation_batch in validation_data_loader:\n",
    "                input, target = validation_batch['input'], validation_batch['target']\n",
    "                output_validation = model(input)\n",
    "                loss = loss_fn(output_validation, target)\n",
    "                loss_validation += loss.item()\n",
    "                num_validations += 1\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training loss\": loss_train / num_trains,\n",
    "            \"Validation loss\": loss_validation / num_validations\n",
    "        })\n",
    "        #100회마다 진행\n",
    "        if epoch >= next_print_epoch:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, \"\n",
    "                f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "                f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "            )\n",
    "            next_print_epoch += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델을 test 하고 submission 생성\n",
    "\n",
    "def test_and_create_submission(model, test_data_loader):\n",
    "    model.eval()  # 모델을 평가 모드로 전환, 가중치를 업데이트 하지 않음\n",
    "    all_predictions = []\n",
    "    passenger_ids = list(range(892, 892 + len(test_data_loader.dataset)))  # Titanic 테스트 데이터의 승객 ID는 892부터 시작\n",
    "\n",
    "    #업데이트를 하지 않기 때문에 no_grad\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_data_loader:\n",
    "            input = test_batch['input']  # TitanicTestDataset을 사용하여 'input' 데이터를 가져옴\n",
    "            output = model(input)\n",
    "            predictions = torch.argmax(output, dim=1).cpu().numpy()  # 0 or 1 중 가장 높은 확률의 클래스를 예측\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "    # submission.csv 생성\n",
    "    submission_df = pd.DataFrame({\n",
    "        #형식에 맞게\n",
    "        'PassengerId': passenger_ids,\n",
    "        'Survived': all_predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('submission_test.csv', index=False)\n",
    "    print(\"submission_test.csv 파일이 생성되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    #현재 시간 기록\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    #모델에 설정할 값들을 담은 딕셔너리\n",
    "    #에포크, 배치사이즈, 러닝 레이트, 히든 유닛 개수 등 포함되어 있음\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': 1e-3,\n",
    "        'n_hidden_unit_list': [20, 20],\n",
    "    }\n",
    "\n",
    "    #wandb 모델의 초기값 생성\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"my_model_training\",\n",
    "        notes=\"Titanic Dataset experiment\",\n",
    "        tags=[\"my_model\", \"titanic\"],\n",
    "        name=current_time_str,\n",
    "        config=config\n",
    "    )\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    #모델과 옵티마이저 생성\n",
    "    model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    #모델 train 시작.\n",
    "    training_loop(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_data_loader=train_data_loader,\n",
    "        validation_data_loader=validation_data_loader\n",
    "    )\n",
    "    #train 끝\n",
    "    #test 실행\n",
    "    test_and_create_submission(model, test_data_loader)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33majhajh503\u001b[0m (\u001b[33majhajh503-korea-university-of-technology-and-education\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ajhaj\\git\\link_dl\\_04_homeworks_solution\\homework_2\\wandb\\run-20241018_024229-n0gp774x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/n0gp774x' target=\"_blank\">2024-10-18_02-42-29</a></strong> to <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/n0gp774x' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/n0gp774x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001DB6D7057E0>\n",
      "{'epochs': 2000, 'batch_size': 512, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}\n",
      "################################################## 1\n",
      "Epoch 100, Training loss 0.5617, Validation loss 0.5655\n",
      "Epoch 200, Training loss 0.5521, Validation loss 0.5916\n",
      "Epoch 300, Training loss 0.5294, Validation loss 0.5316\n",
      "Epoch 400, Training loss 0.5060, Validation loss 0.5474\n",
      "Epoch 500, Training loss 0.4754, Validation loss 0.5379\n",
      "Epoch 600, Training loss 0.4517, Validation loss 0.4871\n",
      "Epoch 700, Training loss 0.4382, Validation loss 0.5334\n",
      "Epoch 800, Training loss 0.4189, Validation loss 0.4767\n",
      "Epoch 900, Training loss 0.4232, Validation loss 0.4773\n",
      "Epoch 1000, Training loss 0.4232, Validation loss 0.4789\n",
      "Epoch 1100, Training loss 0.4054, Validation loss 0.5427\n",
      "Epoch 1200, Training loss 0.4059, Validation loss 0.5976\n",
      "Epoch 1300, Training loss 0.4005, Validation loss 0.4707\n",
      "Epoch 1400, Training loss 0.3926, Validation loss 0.4882\n",
      "Epoch 1500, Training loss 0.3907, Validation loss 0.5508\n",
      "Epoch 1600, Training loss 0.3816, Validation loss 0.5261\n",
      "Epoch 1700, Training loss 0.4006, Validation loss 0.4712\n",
      "Epoch 1800, Training loss 0.3763, Validation loss 0.4840\n",
      "Epoch 1900, Training loss 0.3829, Validation loss 0.4677\n",
      "Epoch 2000, Training loss 0.3881, Validation loss 0.6080\n",
      "submission_test.csv 파일이 생성되었습니다!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>Training loss</td><td>█████▇▇▇▆▆▅▄▄▃▃▃▃▃▃▃▂▃▃▃▂▃▂▂▃▂▂▂▂▁▂▂▁▁▂▁</td></tr><tr><td>Validation loss</td><td>▅▅▆▅▄▅▅▂▃▄▄▂▃▃▂▁▂▁▂▃▇▃▂▁▁▂▁▂▂▁▅▃▂▄▂▃▃▁▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2000</td></tr><tr><td>Training loss</td><td>0.38812</td></tr><tr><td>Validation loss</td><td>0.608</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-18_02-42-29</strong> at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/n0gp774x' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/n0gp774x</a><br/> View project at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241018_024229-n0gp774x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Jupyter Notebook 환경을 확인하여 argparse를 우회.\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        # 기본값을 설정\n",
    "        class Args:\n",
    "            wandb = True\n",
    "            batch_size = 512\n",
    "            epochs = 2000 # Early stopping을 위해 epochs를 2000으로 설정\n",
    "\n",
    "        args = Args() # args 추가\n",
    "        main(args) # 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu 활성화 함수\n",
    "---\n",
    "https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/77qgbsbu?nw=nwuserajhajh503\n",
    "\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/Relu_training_loss.svg)\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/Relu_V.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ajhaj\\git\\link_dl\\_04_homeworks_solution\\homework_2\\wandb\\run-20241018_024408-qv4ha66j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/qv4ha66j' target=\"_blank\">2024-10-18_02-44-08</a></strong> to <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/qv4ha66j' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/qv4ha66j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001DB71404D30>\n",
      "{'epochs': 2000, 'batch_size': 512, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}\n",
      "################################################## 1\n",
      "Epoch 100, Training loss 0.5589, Validation loss 0.5985\n",
      "Epoch 200, Training loss 0.5267, Validation loss 0.5495\n",
      "Epoch 300, Training loss 0.4993, Validation loss 0.6037\n",
      "Epoch 400, Training loss 0.4664, Validation loss 0.5006\n",
      "Epoch 500, Training loss 0.4634, Validation loss 0.5793\n",
      "Epoch 600, Training loss 0.4379, Validation loss 0.5993\n",
      "Epoch 700, Training loss 0.4257, Validation loss 0.5441\n",
      "Epoch 800, Training loss 0.4109, Validation loss 0.5218\n",
      "Epoch 900, Training loss 0.4007, Validation loss 0.4736\n",
      "Epoch 1000, Training loss 0.3959, Validation loss 0.5473\n",
      "Epoch 1100, Training loss 0.3990, Validation loss 0.4739\n",
      "Epoch 1200, Training loss 0.4033, Validation loss 0.5078\n",
      "Epoch 1300, Training loss 0.3954, Validation loss 0.4806\n",
      "Epoch 1400, Training loss 0.3860, Validation loss 0.5268\n",
      "Epoch 1500, Training loss 0.3813, Validation loss 0.5453\n",
      "Epoch 1600, Training loss 0.3645, Validation loss 0.5358\n",
      "Epoch 1700, Training loss 0.3823, Validation loss 0.5935\n",
      "Epoch 1800, Training loss 0.3737, Validation loss 0.5433\n",
      "Epoch 1900, Training loss 0.3555, Validation loss 0.5661\n",
      "Epoch 2000, Training loss 0.3629, Validation loss 0.5622\n",
      "submission_test.csv 파일이 생성되었습니다!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>Training loss</td><td>█▇▇▄▄▅▄▄▃▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>▄▄▃▄▃▃▄▃▁▃▂▂▂▅▂▂▇▂▁▁▄▁▂▅▁▁▂▂▂▅▃▄▃▁▁▁█▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2000</td></tr><tr><td>Training loss</td><td>0.36293</td></tr><tr><td>Validation loss</td><td>0.56218</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-18_02-44-08</strong> at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/qv4ha66j' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/qv4ha66j</a><br/> View project at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241018_024408-qv4ha66j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ELU로 변경, 그외 전부 동일\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #wandb.config.n_hidden_unit_list[0] 에서 설정된 첫 번째 히든레이어 노드 수\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter Notebook 환경을 확인하여 argparse를 우회.\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        # 기본값을 설정\n",
    "        class Args:\n",
    "            wandb = True\n",
    "            batch_size = 512\n",
    "            epochs = 2000 # Early stopping을 위해 epochs를 2000으로 설정\n",
    "\n",
    "        args = Args() # args 추가\n",
    "        main(args) # 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELU 활성화 함수\n",
    "---\n",
    "https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/qazpje1y?nw=nwuserajhajh503\n",
    "\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/ELU_T.svg)\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/ELU_V.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ajhaj\\git\\link_dl\\_04_homeworks_solution\\homework_2\\wandb\\run-20241018_024544-47e300cq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/47e300cq' target=\"_blank\">2024-10-18_02-45-44</a></strong> to <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/47e300cq' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/47e300cq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001DB71407F40>\n",
      "{'epochs': 2000, 'batch_size': 512, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}\n",
      "################################################## 1\n",
      "Epoch 100, Training loss 0.5718, Validation loss 0.5917\n",
      "Epoch 200, Training loss 0.5533, Validation loss 0.6267\n",
      "Epoch 300, Training loss 0.5342, Validation loss 0.5681\n",
      "Epoch 400, Training loss 0.4921, Validation loss 0.5394\n",
      "Epoch 500, Training loss 0.4626, Validation loss 0.5517\n",
      "Epoch 600, Training loss 0.4328, Validation loss 0.6449\n",
      "Epoch 700, Training loss 0.4255, Validation loss 0.4834\n",
      "Epoch 800, Training loss 0.4306, Validation loss 0.5361\n",
      "Epoch 900, Training loss 0.4231, Validation loss 0.4775\n",
      "Epoch 1000, Training loss 0.4020, Validation loss 0.5486\n",
      "Epoch 1100, Training loss 0.3912, Validation loss 0.6462\n",
      "Epoch 1200, Training loss 0.4127, Validation loss 0.6023\n",
      "Epoch 1300, Training loss 0.4005, Validation loss 0.5501\n",
      "Epoch 1400, Training loss 0.4035, Validation loss 0.8314\n",
      "Epoch 1500, Training loss 0.3953, Validation loss 0.4848\n",
      "Epoch 1600, Training loss 0.3819, Validation loss 0.5396\n",
      "Epoch 1700, Training loss 0.4032, Validation loss 0.4828\n",
      "Epoch 1800, Training loss 0.3887, Validation loss 0.5181\n",
      "Epoch 1900, Training loss 0.3711, Validation loss 0.5947\n",
      "Epoch 2000, Training loss 0.3702, Validation loss 0.6703\n",
      "submission_test.csv 파일이 생성되었습니다!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇████</td></tr><tr><td>Training loss</td><td>███▇▇▇▆▆▆▆▅▅▅▄▄▃▃▄▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>▄▄▅▄▄▄▃█▄▂▂▂▃▃▁▃▃▃▁▂▂▂▆▂▂▂▅▃▂▂▁▁▂▁▁▃▁▄▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2000</td></tr><tr><td>Training loss</td><td>0.37016</td></tr><tr><td>Validation loss</td><td>0.67026</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-18_02-45-44</strong> at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/47e300cq' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/47e300cq</a><br/> View project at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241018_024544-47e300cq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#LeakyReLU로 변경, 그외 전부 동일\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #wandb.config.n_hidden_unit_list[0] 에서 설정된 첫 번째 히든레이어 노드 수\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter Notebook 환경을 확인하여 argparse를 우회.\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        # 기본값을 설정\n",
    "        class Args:\n",
    "            wandb = True\n",
    "            batch_size = 512\n",
    "            epochs = 2000 # Early stopping을 위해 epochs를 2000으로 설정\n",
    "\n",
    "        args = Args() # args 추가\n",
    "        main(args) # 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeakyReLU 활성화 함수\n",
    "---\n",
    "https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/vzofv6wt?nw=nwuserajhajh503\n",
    "\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/Leaky_T.svg)\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/Leaky_V.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ajhaj\\git\\link_dl\\_04_homeworks_solution\\homework_2\\wandb\\run-20241018_024733-ufj393tk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/ufj393tk' target=\"_blank\">2024-10-18_02-47-33</a></strong> to <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/ufj393tk' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/ufj393tk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001DB714F8E50>\n",
      "{'epochs': 2000, 'batch_size': 512, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}\n",
      "################################################## 1\n",
      "Epoch 100, Training loss 0.5668, Validation loss 0.5775\n",
      "Epoch 200, Training loss 0.5525, Validation loss 0.6186\n",
      "Epoch 300, Training loss 0.5248, Validation loss 0.5759\n",
      "Epoch 400, Training loss 0.5058, Validation loss 0.5006\n",
      "Epoch 500, Training loss 0.4826, Validation loss 0.4879\n",
      "Epoch 600, Training loss 0.4659, Validation loss 0.5259\n",
      "Epoch 700, Training loss 0.4404, Validation loss 0.4956\n",
      "Epoch 800, Training loss 0.4320, Validation loss 0.5646\n",
      "Epoch 900, Training loss 0.4256, Validation loss 0.4869\n",
      "Epoch 1000, Training loss 0.4396, Validation loss 0.4716\n",
      "Epoch 1100, Training loss 0.4278, Validation loss 0.5464\n",
      "Epoch 1200, Training loss 0.4061, Validation loss 0.4723\n",
      "Epoch 1300, Training loss 0.4011, Validation loss 0.4616\n",
      "Epoch 1400, Training loss 0.3855, Validation loss 0.4764\n",
      "Epoch 1500, Training loss 0.3840, Validation loss 0.5716\n",
      "Epoch 1600, Training loss 0.3863, Validation loss 0.5703\n",
      "Epoch 1700, Training loss 0.3694, Validation loss 0.5042\n",
      "Epoch 1800, Training loss 0.3920, Validation loss 0.4656\n",
      "Epoch 1900, Training loss 0.3638, Validation loss 0.4812\n",
      "Epoch 2000, Training loss 0.3502, Validation loss 0.5254\n",
      "submission_test.csv 파일이 생성되었습니다!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>Training loss</td><td>███▇▇▇▇▇▆▇▅▄▄▅▄▄▃▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁</td></tr><tr><td>Validation loss</td><td>▅▅▄▅▅▅▅▄▅▄▃▅▅▂█▃▄▁▂▂▁▁▁▁▄▂▄▂▁█▆▁▂▂▃▁▃▃▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2000</td></tr><tr><td>Training loss</td><td>0.35018</td></tr><tr><td>Validation loss</td><td>0.52537</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-18_02-47-33</strong> at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/ufj393tk' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/ufj393tk</a><br/> View project at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241018_024733-ufj393tk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PReLU로 변경, 그외 전부 동일\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #wandb.config.n_hidden_unit_list[0] 에서 설정된 첫 번째 히든레이어 노드 수\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.PReLU(num_parameters=1, init=0.25),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.PReLU(num_parameters=1, init=0.25),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter Notebook 환경을 확인하여 argparse를 우회.\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        # 기본값을 설정\n",
    "        class Args:\n",
    "            wandb = True\n",
    "            batch_size = 512\n",
    "            epochs = 2000 # Early stopping을 위해 epochs를 2000으로 설정\n",
    "\n",
    "        args = Args() # args 추가\n",
    "        main(args) # 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PReLU 활성화 함수\n",
    "---\n",
    "https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/3mhrerpw?nw=nwuserajhajh503\n",
    "\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/PRelu_T.svg)\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/PRelu_V.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[요구 사항3]\n",
    "---\n",
    "테스트 및 submission.csv 생성\n",
    "\n",
    "오버피팅으로 인해 Activation Function 차이를 보기 힘들었음.\n",
    "\n",
    "따라서 제일 가성비 좋은 기본 Relu를 사용해서 모델을 구성하고, 수업시간에 배운 Early Stopping을 사용해\n",
    "\n",
    "Validation이 지속해서 증가하면 훈련을 멈추게 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ajhaj\\git\\link_dl\\_04_homeworks_solution\\homework_2\\wandb\\run-20241018_024917-7cfdilmc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/7cfdilmc' target=\"_blank\">2024-10-18_02-49-17</a></strong> to <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/7cfdilmc' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/7cfdilmc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001DB71542DD0>\n",
      "{'epochs': 2000, 'batch_size': 512, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}\n",
      "################################################## 1\n",
      "Epoch 100, Training loss 0.5717, Validation loss 0.5569\n",
      "Epoch 200, Training loss 0.5581, Validation loss 0.5758\n",
      "Epoch 300, Training loss 0.5438, Validation loss 0.5753\n",
      "Epoch 400, Training loss 0.5261, Validation loss 0.6289\n",
      "Epoch 500, Training loss 0.4998, Validation loss 0.5295\n",
      "Epoch 600, Training loss 0.4732, Validation loss 0.4898\n",
      "Epoch 700, Training loss 0.4526, Validation loss 0.5931\n",
      "Epoch 800, Training loss 0.4337, Validation loss 0.4789\n",
      "Epoch 900, Training loss 0.4142, Validation loss 0.5368\n",
      "Epoch 1000, Training loss 0.4186, Validation loss 0.5575\n",
      "Early stopping at epoch 1070. Best validation loss: 0.4487\n",
      "submission_test.csv 파일이 생성되었습니다!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>Training loss</td><td>████▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▄▃▄▃▃▃▃▂▃▂▂▃▂▂▂▂▂▁▁▁</td></tr><tr><td>Validation loss</td><td>▆▇▆▇▆▇▆▅▄▅▆█▅▄▅▄▅▅▄▃▃▇▂▃▃▂▂▂▃▂▅▄▃▃▁▁▅▃▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1070</td></tr><tr><td>Training loss</td><td>0.41197</td></tr><tr><td>Validation loss</td><td>0.57084</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-18_02-49-17</strong> at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/7cfdilmc' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/7cfdilmc</a><br/> View project at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241018_024917-7cfdilmc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader, args):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    best_validation_loss = float('inf')\n",
    "    patience_counter = 0  # Early stopping을 위한 patience 카운터\n",
    "    next_print_epoch = 100 # 100번마다 출력\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()  # 학습 모드 전환\n",
    "        loss_train = 0.0\n",
    "        num_trains = 0\n",
    "        for train_batch in train_data_loader:\n",
    "            input, target = train_batch['input'], train_batch['target']\n",
    "            output_train = model(input)\n",
    "            loss = loss_fn(output_train, target)\n",
    "            loss_train += loss.item()\n",
    "            num_trains += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % args.validation_intervals == 0:  # 10번마다 검증 실행\n",
    "            model.eval()  # 평가 모드 전환\n",
    "            loss_validation = 0.0\n",
    "            num_validations = 0\n",
    "            with torch.no_grad():\n",
    "                for validation_batch in validation_data_loader:\n",
    "                    input, target = validation_batch['input'], validation_batch['target']\n",
    "                    output_validation = model(input)\n",
    "                    loss = loss_fn(output_validation, target)\n",
    "                    loss_validation += loss.item()\n",
    "                    num_validations += 1\n",
    "\n",
    "            avg_validation_loss = loss_validation / num_validations\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Training loss\": loss_train / num_trains,\n",
    "                \"Validation loss\": avg_validation_loss\n",
    "            })\n",
    "\n",
    "            # Early stopping 조건 확인\n",
    "            if avg_validation_loss + 0.00001< best_validation_loss:\n",
    "                best_validation_loss = avg_validation_loss\n",
    "                patience_counter = 0  # 성능이 개선되면 patience 초기화\n",
    "            else:\n",
    "                patience_counter += 1  # 개선되지 않으면 카운터 증가\n",
    "\n",
    "            if patience_counter >= args.early_stop_patience:  # patience 초과 시 학습 종료\n",
    "                print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_validation_loss:.4f}\")\n",
    "                break\n",
    "            if epoch >= next_print_epoch:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}, \"\n",
    "                    f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "                    f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "                )\n",
    "                next_print_epoch += 100\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #wandb.config.n_hidden_unit_list[0] 에서 설정된 첫 번째 히든레이어 노드 수\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "def main(args):\n",
    "    #현재 시간 기록\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    #모델에 설정할 값들을 담은 딕셔너리\n",
    "    #에포크, 배치사이즈, 러닝 레이트, 히든 유닛 개수 등 포함되어 있음\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': 1e-3,\n",
    "        'n_hidden_unit_list': [20, 20],\n",
    "    }\n",
    "\n",
    "    #wandb 모델의 초기값 생성\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"my_model_training\",\n",
    "        notes=\"Titanic Dataset experiment\",\n",
    "        tags=[\"my_model\", \"titanic\"],\n",
    "        name=current_time_str,\n",
    "        config=config\n",
    "    )\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    #모델과 옵티마이저 생성\n",
    "    model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    #모델 train 시작.\n",
    "    training_loop(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_data_loader=train_data_loader,\n",
    "        validation_data_loader=validation_data_loader,\n",
    "        args=args # args 추가\n",
    "    )\n",
    "    #train 끝\n",
    "    #test 실행\n",
    "    test_and_create_submission(model, test_data_loader)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        # 기본값을 설정\n",
    "        class Args:\n",
    "            wandb = True\n",
    "            batch_size = 512\n",
    "            epochs = 2000 # Early stopping을 위해 epochs를 2000으로 설정\n",
    "            validation_intervals = 10\n",
    "            early_stop_patience = 20\n",
    "\n",
    "        args = Args() # args 추가\n",
    "        main(args) # 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early-Stop\n",
    "---\n",
    "https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/dybp8t2f?nw=nwuserajhajh503\n",
    "\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/Early_T.svg)\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/Early_V.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[요구 사항4]\n",
    "---\n",
    "submission.csv 제출 및 등수확인\n",
    "\n",
    "오버피팅을 줄이기 위해 다양한 정규화 추가\n",
    "변경된 사항으로는 히든레이어 추가, 옵티마이저 변경, 배치 정규화, 드롭 아웃, 하이퍼파라미터 조정 등 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ajhaj\\git\\link_dl\\_04_homeworks_solution\\homework_2\\wandb\\run-20241018_025005-bg6gou1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/bg6gou1o' target=\"_blank\">2024-10-18_02-50-05</a></strong> to <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/bg6gou1o' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/bg6gou1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x000001DB6E8D6920>\n",
      "{'epochs': 1500, 'batch_size': 512, 'learning_rate': 0.0005, 'n_hidden_unit_list': [64, 32, 16]}\n",
      "################################################## 1\n",
      "Epoch 100, Training loss 0.4345, Validation loss 0.5794\n",
      "Epoch 200, Training loss 0.3957, Validation loss 0.4565\n",
      "Epoch 300, Training loss 0.3987, Validation loss 0.4923\n",
      "Epoch 400, Training loss 0.3957, Validation loss 0.4495\n",
      "Epoch 500, Training loss 0.3897, Validation loss 0.4936\n",
      "Epoch 600, Training loss 0.3942, Validation loss 0.4515\n",
      "Epoch 700, Training loss 0.3785, Validation loss 0.4574\n",
      "Epoch 800, Training loss 0.3671, Validation loss 0.5478\n",
      "Early stopping at epoch 900. Best validation loss: 0.4495\n",
      "submission_test.csv 파일이 생성되었습니다!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>Training loss</td><td>█▅▅▄▄▄▃▂▃▃▂▃▃▃▃▂▃▃▂▂▃▂▂▂▂▂▂▂▁▂▁▃▂▁▃▂▁▁▂▁</td></tr><tr><td>Validation loss</td><td>█▆▂▃▄▂▁▃▄▁▁▁▃▁▂▁█▇▁▁▅▃▃▅▁▁▁▃▁▂▁▂▅▃▂▄▂▅▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>900</td></tr><tr><td>Training loss</td><td>0.3679</td></tr><tr><td>Validation loss</td><td>0.46997</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-18_02-50-05</strong> at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/bg6gou1o' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training/runs/bg6gou1o</a><br/> View project at: <a href='https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/ajhajh503-korea-university-of-technology-and-education/my_model_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241018_025005-bg6gou1o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),  # n_input을 실제 입력 크기로 수정\n",
    "            nn.BatchNorm1d(wandb.config.n_hidden_unit_list[0]), # batch_normalziation 추가\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.BatchNorm1d(wandb.config.n_hidden_unit_list[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            #히든 레이어 추가\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], wandb.config.n_hidden_unit_list[2]),\n",
    "            nn.BatchNorm1d(wandb.config.n_hidden_unit_list[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[2], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "def get_model_and_optimizer():\n",
    "    n_input = 11\n",
    "    n_output = 2\n",
    "    model = MyModel(n_input=n_input, n_output=n_output)\n",
    "\n",
    "    #SGD에서 ADAM으로 변경\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    return model, optimizer\n",
    "\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader, args):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    best_validation_loss = float('inf')\n",
    "    patience_counter = 0  # Early stopping을 위한 patience 카운터\n",
    "    next_print_epoch = 100\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()  # 학습 모드 전환\n",
    "        loss_train = 0.0\n",
    "        num_trains = 0\n",
    "        for train_batch in train_data_loader:\n",
    "            input, target = train_batch['input'], train_batch['target']\n",
    "            output_train = model(input)\n",
    "            loss = loss_fn(output_train, target)\n",
    "            loss_train += loss.item()\n",
    "            num_trains += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % args.validation_intervals == 0:  # 10번마다 검증 실행\n",
    "            model.eval()  # 평가 모드 전환\n",
    "            loss_validation = 0.0\n",
    "            num_validations = 0\n",
    "            with torch.no_grad():\n",
    "                for validation_batch in validation_data_loader:\n",
    "                    input, target = validation_batch['input'], validation_batch['target']\n",
    "                    output_validation = model(input)\n",
    "                    loss = loss_fn(output_validation, target)\n",
    "                    loss_validation += loss.item()\n",
    "                    num_validations += 1\n",
    "\n",
    "            avg_validation_loss = loss_validation / num_validations\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Training loss\": loss_train / num_trains,\n",
    "                \"Validation loss\": avg_validation_loss\n",
    "            })\n",
    "\n",
    "            # Early stopping 조건 확인\n",
    "            if avg_validation_loss + 0.00001< best_validation_loss:\n",
    "                best_validation_loss = avg_validation_loss\n",
    "                patience_counter = 0  # 성능이 개선되면 patience 초기화\n",
    "            else:\n",
    "                patience_counter += 1  # 개선되지 않으면 카운터 증가\n",
    "\n",
    "            if patience_counter >= args.early_stop_patience:  # patience 초과 시 학습 종료\n",
    "                print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_validation_loss:.4f}\")\n",
    "                break\n",
    "            if epoch >= next_print_epoch:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}, \"\n",
    "                    f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "                    f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "                )\n",
    "                next_print_epoch += 100\n",
    "\n",
    "def test_and_create_submission(model, test_data_loader):\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    all_predictions = []\n",
    "    passenger_ids = list(range(892, 892 + len(test_data_loader.dataset)))  # Titanic 테스트 데이터의 승객 ID는 892부터 시작\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_data_loader:\n",
    "            input = test_batch['input']  # TitanicTestDataset을 사용하여 'input' 데이터를 가져옴\n",
    "            output = model(input)\n",
    "            predictions = torch.argmax(output, dim=1).cpu().numpy()  # 가장 높은 확률의 클래스를 예측\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "    # submission.csv 생성\n",
    "    submission_df = pd.DataFrame({\n",
    "        'PassengerId': passenger_ids,\n",
    "        'Survived': all_predictions\n",
    "    })\n",
    "    submission_df.to_csv('submission_test.csv', index=False)\n",
    "    print(\"submission_test.csv 파일이 생성되었습니다!\")\n",
    "\n",
    "def main(args):\n",
    "    #현재 시간 기록\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    #모델에 설정할 값들을 담은 딕셔너리\n",
    "    #에포크, 배치사이즈, 러닝 레이트, 히든 유닛 개수 등 포함되어 있음\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': 5e-4,\n",
    "        'n_hidden_unit_list': [64, 32, 16],\n",
    "    }\n",
    "\n",
    "    #wandb 모델의 초기값 생성\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"my_model_training\",\n",
    "        notes=\"Titanic Dataset experiment\",\n",
    "        tags=[\"my_model\", \"titanic\"],\n",
    "        name=current_time_str,\n",
    "        config=config\n",
    "    )\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    #모델과 옵티마이저 생성\n",
    "    model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    #모델 train 시작.\n",
    "    training_loop(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_data_loader=train_data_loader,\n",
    "        validation_data_loader=validation_data_loader,\n",
    "        args=args # args 추가\n",
    "    )\n",
    "    #train 끝\n",
    "    #test 실행\n",
    "    test_and_create_submission(model, test_data_loader)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        class Args:\n",
    "            wandb = True\n",
    "            batch_size = 512\n",
    "            epochs = 1500 # Early stopping을 위해 epochs를 1000으로 설정\n",
    "            validation_intervals = 10\n",
    "            early_stop_patience = 50\n",
    "\n",
    "        args = Args() # args 추가\n",
    "        main(args) # 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission\n",
    "---\n",
    "\n",
    "![Relu Activation Function](https://raw.githubusercontent.com/ajh1004ajh00/link_dl/main/_04_homeworks_solution/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[숙제 후기]\n",
    "---\n",
    "titanic dataset을 이용해 모델을 직접 설계해보는 실습을 해 보았습니다.\n",
    "\n",
    "제일 기억에 남는것은 Loss는 감소하는 반면 validation Loss는 개선되지 않는 오버피팅 현상을 겪었습니다.\n",
    "\n",
    "이를 해결하기 위해 Batch Normalization, Dropout, 하이퍼파라미터 조정 등 다양하게 해 보았지만 성능차이가 크게 나지 않았습니다.\n",
    "\n",
    "처음보단 오버피팅을 완화하긴 했지만 그 차이가 크지 않아 아쉬웠습니다.\n",
    "\n",
    "이번 과제를 통해 딥러닝 모델의 학습 과정에서 발생할 수 있는 문제들을 이해하고,\n",
    "이를 해결하기 위한 다양한 기법들을 실제로 적용해 볼 수 있었습니다.\n",
    "또한, 하이퍼파라미터 튜닝이 진짜 어렵다는 것을 알게된 시간이였습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
